import { OpenAIGPTChat } from "@/components/api/schemas/OpenAIGPTChat";
import { getAuthToken } from "@/lib/auth-integration";
import { useMutation } from "@tanstack/react-query";

/**
 * Message role types for OpenAI chat conversations
 */
export type ChatMessageRole = "system" | "user" | "assistant";

/**
 * Individual message in the conversation history
 */
export interface ChatMessage {
	/** Role of the message author */
	role: ChatMessageRole;
	/** Content of the message */
	content: string;
}

/**
 * Input parameters for OpenAI GPT chat completion
 */
export interface OpenAIGPTChatInput {
	/** Array of messages representing the conversation history (minimum 1 message required) */
	messages: ChatMessage[];
	/** Model name to use for completion (default: 'MaaS_4.1') */
	model?: string;
}

/**
 * Message in the assistant's response
 */
export interface AssistantMessage {
	/** Role of the author (always 'assistant') */
	role: "assistant";
	/** Content of the assistant's message */
	content: string;
	/** Reasoning content, if any */
	reasoning_content?: string | null;
	/** Function call generated by the model, if any */
	function_call?: Record<string, unknown> | null;
	/** Tool calls generated by the model, if any */
	tool_calls?: Array<Record<string, unknown>> | null;
	/** Additional reasoning details, if any */
	reasoning_details?: Record<string, unknown> | null;
}

/**
 * Single choice from the chat completion response
 */
export interface ChatChoice {
	/** Index of the choice in the list */
	index: number;
	/** The assistant's message */
	message: AssistantMessage;
	/** Reason the model stopped generating tokens */
	finish_reason:
		| "stop"
		| "length"
		| "function_call"
		| "content_filter"
		| "null";
	/** Log probability information, if requested */
	logprobs?: Record<string, unknown> | null;
	/** Native finish reason from underlying model */
	native_finish_reason?: string | null;
}

/**
 * Token usage statistics for the completion
 */
export interface TokenUsage {
	/** Number of tokens in the prompt */
	prompt_tokens: number;
	/** Number of tokens in the completion */
	completion_tokens: number;
	/** Total tokens used (prompt + completion) */
	total_tokens: number;
	/** Detailed breakdown of completion tokens */
	completion_tokens_details?: {
		/** Number of tokens used for reasoning */
		reasoning_tokens: number;
	};
	/** Detailed breakdown of prompt tokens */
	prompt_tokens_details?: Record<string, unknown> | null;
}

/**
 * Complete chat completion response from OpenAI
 */
export interface OpenAIGPTChatOutput {
	/** Unique identifier for the chat completion */
	id: string;
	/** Object type (always 'chat.completion') */
	object: "chat.completion";
	/** Unix timestamp when the completion was created */
	created: number;
	/** Model used for the chat completion */
	model: string;
	/** Backend configuration fingerprint */
	system_fingerprint?: string | null;
	/** Array of chat completion choices */
	choices: ChatChoice[];
	/** Token usage statistics */
	usage: TokenUsage;
}

/**
 * React hook for OpenAI GPT Chat completions
 *
 * Provides AI-powered chat capabilities using OpenAI's GPT models through Creao's API.
 * Supports conversation history and multiple use cases including:
 *
 * **TrialFlow Use Cases:**
 * 1. Scheduling Optimization: Analyze clinician availability and patient booking patterns
 * 2. Smart Recommendations: Suggest optimal time windows to minimize patient burden
 * 3. Procedure Bundling: Recommend combining procedures (e.g., "blood draw and ECG on same day")
 * 4. Conflict Detection: Identify overlapping procedures or insufficient recovery time
 * 5. Resolution Suggestions: Generate intelligent solutions for scheduling conflicts
 *
 * @example
 * ```tsx
 * const chatCompletion = useOpenAIGPTChat();
 *
 * // Simple user query
 * chatCompletion.mutate({
 *   messages: [
 *     { role: 'user', content: 'What is the capital of France?' }
 *   ]
 * });
 *
 * // TrialFlow: Scheduling conflict detection
 * chatCompletion.mutate({
 *   messages: [
 *     {
 *       role: 'system',
 *       content: 'You are a clinical trial scheduling assistant. Analyze schedules for conflicts and suggest optimal solutions.'
 *     },
 *     {
 *       role: 'user',
 *       content: 'Patient has MRI scheduled on Monday 9am and blood draw on Monday 8:30am. Both take 1 hour. Are there conflicts?'
 *     }
 *   ]
 * });
 *
 * // TrialFlow: Procedure bundling recommendation
 * chatCompletion.mutate({
 *   messages: [
 *     {
 *       role: 'system',
 *       content: 'You are a clinical trial optimization assistant. Recommend procedure bundling to minimize patient visits.'
 *     },
 *     {
 *       role: 'user',
 *       content: 'Patient needs: ECG (15 min), blood draw (10 min), vital signs (5 min). Currently scheduled on 3 different days.'
 *     }
 *   ],
 *   model: 'MaaS_4.1'
 * });
 *
 * // Access the response
 * if (chatCompletion.isSuccess) {
 *   const assistantReply = chatCompletion.data.choices[0].message.content;
 *   const tokensUsed = chatCompletion.data.usage.total_tokens;
 * }
 * ```
 */
export function useOpenAIGPTChat() {
	return useMutation<OpenAIGPTChatOutput, Error, OpenAIGPTChatInput>({
		mutationFn: async (
			params: OpenAIGPTChatInput,
		): Promise<OpenAIGPTChatOutput> => {
			// Validate required parameters
			if (!params.messages || params.messages.length === 0) {
				throw new Error(
					"At least one message is required in the messages array",
				);
			}

			// Validate message structure
			for (const message of params.messages) {
				if (!message.role || !message.content) {
					throw new Error("Each message must have a valid role and content");
				}
				if (!["system", "user", "assistant"].includes(message.role)) {
					throw new Error(
						`Invalid message role: ${message.role}. Must be 'system', 'user', or 'assistant'`,
					);
				}
			}

			// Initialize API client with authentication
			const apiClient = new OpenAIGPTChat({
				TOKEN: getAuthToken() || "",
			});

			// Call the chat completion API
			const response = await apiClient.default.createChatCompletion({
				model: params.model || "MaaS_4.1",
				messages: params.messages,
			});

			return response;
		},
		retry: 2,
		retryDelay: (attemptIndex) => Math.min(1000 * 2 ** attemptIndex, 30000),
	});
}
